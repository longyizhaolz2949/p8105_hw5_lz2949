---
title: "p8105_hw5_lz2949"
author: "Longyi Zhao"
date: "2023-11-03"
output: github_document
---

```{r}
library(tidyverse)
library(broom)
library(ggplot2)
```

## Problem 1
```{r}
homi_data = read_csv(file = "./data/homicide-data.csv")
```

```{r}
# Create a city_state variable
homi_data  = 
  homi_data |>
  mutate(city_state = paste(city,state,sep = ", "))

# summary and count number of homicides 
summary_data = homi_data |>
  group_by(city_state) |>
  summarise(
    homi_solved = sum(disposition == 'Closed by arrest'),
    homi_unsolved = sum(disposition == 'Open/No arrest' | disposition == 'Closed without arrest')
  )

```
Description: 

```{r}
# in 'Baltimore, MD' estimate the proportion of homicides that are unsolved. 
balt_md = summary_data |>
  filter(city_state == "Baltimore, MD") |>
  mutate(sum_homi = sum(homi_solved, homi_unsolved)) |>
  do ({prop.test(.$homi_unsolved, .$sum_homi) |>
  broom::tidy() })|>
  select(estimate, conf.low, conf.high) |>
  mutate(city_state = "Baltimore, MD") 
```

```{r}
# run prop.test for each of the city
result_df <- summary_data |>
  group_by(city_state) |>
  summarize(prop_test_results = map(homi_unsolved, ~ prop.test(.x, .x +homi_solved) |>
  broom::tidy())) |>
  unnest(prop_test_results) |>
  select(city_state, estimate, conf.low, conf.high) 

print(result_df)
```

```{r}
ggplot(result_df, aes(x = city_state, 
                      y = estimate, 
                      color = city_state
                      )) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Estimated Proportions and Confidence Intervals by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  theme(legend.position = "none")

# drop legend?
```
description 

## Problem 2
```{r}
# a dataframe containign all file names
data_directory = "./datap2"
file_names = list.files(path = data_directory, full.names = TRUE, pattern = ".csv")

# iterate over file names and read in data for each subject using map and save rthe result
data_list = map(file_names, ~ read.csv(.x) |>
                  mutate(subject_id = basename(.x)) |>
                  separate(subject_id, c("subject_id", "arm"), sep = "_"))

# tidy the result
tidy_data = bind_rows(data_list) |>
  rename(test_arm = subject_id, test_id = arm) |>
  mutate(test_id = sub( "\\.csv$", "", test_id)) |>
  pivot_longer(cols = starts_with("week_"), names_to = "time", values_to = "value")

```

```{r}
# make a spaghetti plot showing observations on each subject over time
ggplot(tidy_data, 
       aes(x = time, y = value, group = test_id, color = test_id)) +
  geom_point() +
  geom_line() +
  facet_wrap(.~test_arm) +
  labs(
    title = "Spaghetti Plot of Observations Over Time",
    x = "Time",
    y = "Value",
    color = "Subject ID"
  ) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```
Description: In the control group, though each participant has some fluctuations in 8 weeks, values are generally stable, no clear trend of increasing or decreasing. The average value of 10 participants in control group at week 1 is `r mean(tidy_data |>filter(test_arm == "con", time == "week_1") |>pull(value))`; the average value of 10 participants in control group at week 8 is `r mean(tidy_data |>filter(test_arm == "con", time == "week_8") |>pull(value))`. In the experiment group, there is a clear increasing trend as time passed: at week 1, the average value is `r mean(tidy_data |>filter(test_arm == "exp", time == "week_1") |>pull(value))`; at week 8, the average value is `r mean(tidy_data |>filter(test_arm == "exp", time == "week_8") |>pull(value))`. 

## Problem 3
Conduct a simulation to explore power in a one-sample t-test
```{r}
# fix two design elements
# alpha = 0.95
# H0, true mean is equal to 0 
n = 30
sigma = 5
set.seed(12345)

# set mu = 0 and generate 5000 datasets from the model x~norm(mu, sigma)
mu = 0

output = vector("list", length= 5000)

for (i in 1:5000) {
  sample_data = rnorm(n, mean = mu, sd = sigma)
  output[[i]] = t.test(sample_data, mu = 0) |>
    broom::tidy()
}

sim_results = output |>
  bind_rows() |>
  select(estimate, p.value) |>
  mutate(mu = 0) 
```

```{r}
# repeat the result for mu = {1,2,3,4,5,6}
simulate_mu = function(mu) {
  output_onesix = vector("list", length = 5000)

  for (i in 1:5000) {
    sample_data_onesix = rnorm(n, mean = mu, sd = sigma)
    output_onesix[[i]] = t.test(sample_data_onesix, mu = 0) |> 
      broom::tidy()
  }

  sim_results_onesix = output_onesix |> 
    bind_rows() |> 
    select(estimate, p.value) |>
    mutate(mu = mu)
  return(sim_results_onesix)
}

mu_onesix = c(1,2,3,4,5,6)
results_onesix = map(mu_onesix, simulate_mu) |>
  bind_rows() 

```

Make a plot showing the proportion of times the null was rejected on the y axis and true value of mu on the x axis
```{r}
reject_data = results_onesix |>
  group_by (mu) |>
  mutate(rejected = p.value <0.05) |>
  summarize(proportion_rejected = sum(rejected)/n())

ggplot(reject_data, aes (x = mu, y = proportion_rejected)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True Value of mu",
    y = "Proportion of Rejections (Power)",
    title = "Power vs. True Value of mu"
  ) +
  scale_x_continuous(breaks = seq(0, 6, by = 1)) 
```
Description: From the plot, when mu increase, the proportion of rejection(power) increases. When mu is 5 and 6, the power is almost 1. Statistically speaking, larger effect size would lead to higher power because larger effect size means the differences between groups are bigger, easier to detect, leading to higher rejection rate. 

Make a plot showing the average estimate of mu_hat on the y axis and true value of mu on the x axis
```{r}
average_mu = results_onesix |>
  group_by(mu) |>
  summarize(average_mu_hat = mean(estimate))

plot_1 = ggplot (average_mu, aes(x = mu, y = average_mu_hat)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True Value of mu",
    y = "Average Estimate of mu_hat",
    title = "Average Estimate of mu_hat vs. True Value of mu") +
  scale_x_continuous(breaks = seq(0, 6, by = 1)) +
  scale_y_continuous(breaks = seq(0, 6, by = 1)) +
  geom_text(aes(label = sprintf("%.2f", average_mu_hat)), vjust = -0.5)

plot_1
# text label the average estimate of mu hat
```


Make a plot on showing the average estimate of mu_hat only in samples for which the null was rejected on the y axis and the true value of mu on the x axis. 
```{r}
average_reject = results_onesix |>
  group_by(mu) |>
  filter(p.value <0.05) |>
  summarize (average_mu_rej = mean(estimate)) 

plot_2 = ggplot(average_reject, aes(x = mu, y = average_mu_rej)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True Value of mu",
    y = "Average Estimate of mu_hat (Rejected Samples)",
    title = "Average Estimate of mu_hat in Rejected Samples vs. True Value of mu") +
  scale_x_continuous(breaks = seq(0, 6, by = 1)) +
  scale_y_continuous(breaks = seq(0, 6, by = 1)) +
  geom_text(aes(label = sprintf("%.2f", average_mu_rej)), vjust = -0.5)

plot_2
```

```{r}
# if Make two plots overlap
ggplot() + 
  geom_line(data = average_mu, aes(x = mu, y = average_mu_hat, color = 'blue')) + 
  geom_point(data = average_mu, aes(x = mu, y = average_mu_hat, color = 'blue')) +
  geom_line(data = average_reject, aes(x = mu, y = average_mu_rej, color = 'red'))+
  geom_point(data = average_reject, aes(x = mu, y = average_mu_rej, color = 'red')) +
  labs(
    x = "True Value of mu",
    y = "Average Estimate of mu_hat",
    title = "Comparison of Average Estimate of mu_hat in all samples and Rejected Samples"
  ) +
  scale_x_continuous(breaks = seq(0, 6, by = 1)) +
  scale_y_continuous(breaks = seq(0, 6, by = 1)) +
  labs(color = "Legend") +
  scale_color_manual(labels = c("all sample", "rejected sample"), values = c("blue", "red"))

```
Description: From the second plot, the average value of mu_hat is very close to the true value of mu in all samples. For example, when the true value of mu is 1, the average mu_hat is 1.01; when the true value is 5, the average mu_hat is 5.02. 

In rejected samples, at larger mu values (4,5,6), the average estimated mu_hat values are very close to true values. This is because when the power is high, the effect size is larger. However, at smaller mu values, the deviation is larger. For example, when the true mu value is 1, the average estimated mu_hat is 2.27; when true mu value is 3, the average estimated mu_hat is 3.20. This is because at lower power, the effect size is also smaller. 




